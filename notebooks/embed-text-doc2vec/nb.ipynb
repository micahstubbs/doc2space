{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embed-text-doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, let's install some dependencies. a guide to doing this: https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .........\n",
      "Solving package specifications: .\n",
      "\n",
      "# All requested packages already installed.\n",
      "# packages in environment at /Users/m/anaconda3/envs/parse-html:\n",
      "#\n",
      "gensim                    2.3.0               np113py36_0  \n",
      "nltk                      3.2.4                    py36_0  \n"
     ]
    }
   ],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s prepare data for training our doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/'\n",
    "\n",
    "# our list of documents\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "txt_files = glob.glob(f\"{data_dir}/*.txt\")\n",
    "print(len(txt_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trieu et al. - 2017 - News Classification from Social Media Using Twitte.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should an example of just the filename without the path\n",
    "txt_files[0][11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "for file in txt_files:\n",
    "    with open(file, 'r', encoding=\"utf-8\") as file:\n",
    "        currentText = file.read()\n",
    "        data.append(currentText)\n",
    "        file.close()\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2016.2557324, IEEE\n",
      "Transactions on Knowledge and Data Engineering\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 11, NO. 12, 2015\n",
      "\n",
      "1\n",
      "\n",
      "Interactive Visualization\n",
      "of Large Data Sets\n",
      "Parke Godfrey ∗ , Jarek Gryz ∗ , Piotr Lasek ∗ †\n",
      "{godfrey, jarek, plasek}@cse.yorku.ca\n",
      "York University, Canada ∗\n",
      "Rzeszów University, Poland †\n",
      "Abstract—Visualization provides a powerful means for data analysis. But to be practical, visual analytics tools must support smooth\n",
      "and flexible use of visualizations at a fast rate. This becomes increasingly onerous with the ever-increasing size of real-world datasets.\n",
      "First, large databases make interaction more difficult once query response time exceeds several seconds. Second, any attempt to show\n",
      "all data points will overload the visualization, resulting in ch\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "random_index = randrange(len(data)-1)\n",
    "\n",
    "# print the first 1000 characters of a random document from our corpus\n",
    "print(data[random_index][0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), \n",
    "    tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a list of four sentences as training data. Now I have tagged the data and its ready for training. Lets start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha,\n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print ('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model d2v.model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: dm defines the training algorithm. If dm=1 means ‘distributed memory’ (PV-DM) and dm =0 means ‘distributed bag of words’ (PV-DBOW). Distributed Memory model preserves the word order in a document whereas Distributed Bag of words just uses the bag of words approach, which doesn’t preserve any word order.\n",
    "\n",
    "So we have saved the model and it’s ready for implementation. Lets play with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "#to find the vector of a document which is not in the training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find vector of doc in training data using tags\n",
    "# or in other words printing the vector of the document \n",
    "# at index 1 in the training data\n",
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many dimensions does our doc2vec document space have?\n",
    "dimensions = len(model.docvecs['1'])\n",
    "print(dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! This dimensionality is determined by the `vec_size` parameter we specified at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column headers for csv file\n",
    "headers = ['doc']\n",
    "i = 0\n",
    "while i < dimensions:\n",
    "    headers.append(f\"v{i}\")\n",
    "    i+=1\n",
    "    \n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve vectors of all documents in training data\n",
    "# write vectors to a csv file\n",
    "import csv\n",
    "\n",
    "with open('document-vectors.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"')\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    index_count = len(data)-1\n",
    "    i = 0\n",
    "    while i <= index_count:\n",
    "        doc_name = txt_files[i][11:]\n",
    "        vec = list(model.docvecs[i])\n",
    "        row = [doc_name] + vec\n",
    "        writer.writerow(row)\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['doc', 'v0', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19'], ['Trieu et al. - 2017 - News Classification from Social Media Using Twitte.txt', '37.8572', '9.51042', '-12.9113', '-15.5441', '33.3247', '-21.667', '-26.5227', '5.00173', '-6.14793', '48.2965', '50.8205', '15.4903', '-5.50221', '-20.1339', '-38.5444', '5.88733', '6.06908', '9.33432', '-25.3571', '-12.2529']]\n"
     ]
    }
   ],
   "source": [
    "# read vectors in from csv file\n",
    "import csv\n",
    "\n",
    "imported_vectors = []\n",
    "\n",
    "with open('document-vectors.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        imported_vectors.append(row)\n",
    "        \n",
    "print(imported_vectors[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project from 20D to 2D with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize t-SNE projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project from 20D to 2D with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize UMAP projection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
